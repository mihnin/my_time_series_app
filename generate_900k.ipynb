{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6059f3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target rows per shop to reach 900,000 total: 69231\n",
      "Generating 69126 new rows for Shop: Shop F\n",
      "Generating 69063 new rows for Shop: Shop H\n",
      "Generating 69070 new rows for Shop: Shop L\n",
      "Generating 69084 new rows for Shop: Shop I\n",
      "Generating 69148 new rows for Shop: Shop A\n",
      "Generating 69054 new rows for Shop: Shop E\n",
      "Generating 69160 new rows for Shop: Shop G\n",
      "Generating 69146 new rows for Shop: Shop K\n",
      "Generating 69162 new rows for Shop: Shop D\n",
      "Generating 69075 new rows for Shop: Shop M\n",
      "Generating 69140 new rows for Shop: Shop B\n",
      "Generating 69146 new rows for Shop: Shop J\n",
      "Generating 69155 new rows for Shop: Shop C\n",
      "\n",
      "Final DataFrame has 899990 rows.\n",
      "Successfully created 'train_data_900k.xlsx' with expanded data.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# Define input and output filenames\n",
    "input_filename = 'train_data.xlsx'\n",
    "output_filename = 'train_data_900k.xlsx'\n",
    "\n",
    "try:\n",
    "    # Load the existing data\n",
    "    df = pd.read_excel(input_filename)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{input_filename}' was not found.\")\n",
    "    print(\"Please make sure 'train_data.xlsx' is in the same directory as the script.\")\n",
    "    exit()\n",
    "\n",
    "# Ensure 'Date' column is in datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Calculate the target number of rows per shop to reach 900k total\n",
    "# We'll distribute the 900k rows as evenly as possible among existing shops.\n",
    "# If some shops have very few existing rows, they'll get more new rows.\n",
    "# If some shops already have many rows, they'll get fewer new rows.\n",
    "num_shops = df['Shop'].nunique()\n",
    "target_rows_per_shop = 900000 / num_shops\n",
    "print(f\"Target rows per shop to reach 900,000 total: {target_rows_per_shop:.0f}\")\n",
    "\n",
    "# List to hold new data for each shop\n",
    "all_new_data = []\n",
    "\n",
    "# Process each unique shop\n",
    "for shop_name in df['Shop'].unique():\n",
    "    shop_df = df[df['Shop'] == shop_name].copy()\n",
    "\n",
    "    # Get the last date for the current shop\n",
    "    last_date = shop_df['Date'].max()\n",
    "\n",
    "    # Get the unique City and Country for the current shop\n",
    "    # Assuming each shop has a unique City and Country associated with it\n",
    "    shop_city = shop_df['City'].iloc[0]\n",
    "    shop_country = shop_df['Country'].iloc[0]\n",
    "\n",
    "    # Determine how many new rows are needed for this shop\n",
    "    current_shop_rows = len(shop_df)\n",
    "    rows_to_add = int(target_rows_per_shop - current_shop_rows)\n",
    "\n",
    "    if rows_to_add <= 0:\n",
    "        print(f\"Shop '{shop_name}' already has enough rows ({current_shop_rows}). No new rows added for this shop.\")\n",
    "        # If the shop already has enough rows, just add its existing data to the list\n",
    "        all_new_data.append(shop_df)\n",
    "        continue\n",
    "\n",
    "    print(f\"Generating {rows_to_add} new rows for Shop: {shop_name}\")\n",
    "\n",
    "    new_dates = []\n",
    "    new_targets = []\n",
    "    new_cities = []\n",
    "    new_countries = []\n",
    "    new_shops = []\n",
    "\n",
    "    current_date = last_date\n",
    "    for _ in range(rows_to_add):\n",
    "        current_date += timedelta(days=1)  # Increment date by one day\n",
    "        new_dates.append(current_date)\n",
    "        new_targets.append(np.random.randint(200, 1001))  # Random integer between 200 and 1000\n",
    "        new_cities.append(shop_city)\n",
    "        new_countries.append(shop_country)\n",
    "        new_shops.append(shop_name)\n",
    "\n",
    "    # Create a DataFrame for the newly generated data for this shop\n",
    "    new_shop_df = pd.DataFrame({\n",
    "        'City': new_cities,\n",
    "        'Country': new_countries,\n",
    "        'Shop': new_shops,\n",
    "        'Date': new_dates,\n",
    "        'Target': new_targets\n",
    "    })\n",
    "\n",
    "    # Concatenate the existing data with the newly generated data for this shop\n",
    "    combined_shop_df = pd.concat([shop_df, new_shop_df], ignore_index=True)\n",
    "    all_new_data.append(combined_shop_df)\n",
    "\n",
    "# Concatenate all shop dataframes into one final dataframe\n",
    "final_df = pd.concat(all_new_data, ignore_index=True)\n",
    "\n",
    "# Sort by Shop and Date to ensure proper chronological order\n",
    "final_df = final_df.sort_values(by=['Shop', 'Date']).reset_index(drop=True)\n",
    "\n",
    "# Print the total number of rows in the final DataFrame\n",
    "print(f\"\\nFinal DataFrame has {len(final_df)} rows.\")\n",
    "\n",
    "# Save the expanded DataFrame to a new Excel file\n",
    "try:\n",
    "    final_df.to_excel(output_filename, index=False)\n",
    "    print(f\"Successfully created '{output_filename}' with expanded data.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba24ba65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date Country        City    Shop  Target\n",
      "0      2020-01-01  Canada     Toronto  Shop F     456\n",
      "1      2020-01-02  Canada   Vancouver  Shop H     476\n",
      "2      2020-01-03      UK  Manchester  Shop L     162\n",
      "3      2020-01-04  Canada   Vancouver  Shop H     307\n",
      "4      2020-01-05  Canada    Montreal  Shop I     262\n",
      "...           ...     ...         ...     ...     ...\n",
      "899995 2028-01-17  Canada     Toronto  Shop F     348\n",
      "899996 2028-01-18  Canada   Vancouver  Shop H     305\n",
      "899997 2028-01-19      UK  Manchester  Shop L     107\n",
      "899998 2028-01-20  Canada    Montreal  Shop I     112\n",
      "899999 2028-01-21     USA    New York  Shop A     168\n",
      "\n",
      "[900000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01294833",
   "metadata": {},
   "source": [
    "Installing collected packages: webencodings, sortedcontainers, fastjsonschema, zict, widgetsnbextension, websocket-client, webcolors, uri-template, types-python-dateutil, tomli, tinycss2, tblib, send2trash, rfc3986-validator, rfc3339-validator, pywinpty, python-json-logger, pycparser, pandocfilters, overrides, mistune, locket, jupyterlab_widgets, jupyterlab-pygments, jsonpointer, json5, httpcore, fqdn, dataframe-api-compat, bleach, babel, async-lru, terminado, partd, cffi, arrow, modin, jupyter-server-terminals, isoduration, ipywidgets, httpx, dask, argon2-cffi-bindings, nbformat, jupyter-console, distributed, argon2-cffi, nbclient, jupyter-events, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, jupyter, modin-spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a41df792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Parallel `read_excel` is a new feature! If you run into any problems, please visit https://github.com/modin-project/modin/issues. If you find a new issue and can't file it on GitHub, please email bug_reports@modin.org.\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel('train_data_900k.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d2c63a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5dbc42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('train_data_900k.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f7c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Параметры временного ряда\n",
    "num_hours = 200  # количество точек\n",
    "start_time = pd.Timestamp('2024-01-01 00:00:00')\n",
    "id_value = 1\n",
    "\n",
    "# Генерируем последовательность времени с пропусками\n",
    "all_hours = pd.date_range(start=start_time, periods=num_hours, freq='H')\n",
    "# Случайно выберем индексы для пропуска (например, пропустим 20% часов)\n",
    "np.random.seed(42)\n",
    "mask = np.random.rand(num_hours) > 0.2\n",
    "hours_with_gaps = all_hours[mask]\n",
    "\n",
    "# Генерируем значения target\n",
    "targets = np.random.randint(100, 200, size=len(hours_with_gaps))\n",
    "\n",
    "# Собираем DataFrame\n",
    "df_hour = pd.DataFrame({\n",
    "    'id': id_value,\n",
    "    'target': targets,\n",
    "    'time': hours_with_gaps\n",
    "})\n",
    "\n",
    "# Сохраняем в Excel\n",
    "df_hour.to_excel('hour_pred.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
